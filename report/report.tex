%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%packages
\usepackage{amsmath,amssymb,graphicx,amsfonts,psfrag,layout,subfigure,array,longtable,lscape,booktabs,dcolumn,hyperref,multirow}
\usepackage{tabularx}
\usepackage[page]{appendix}
\usepackage{multicol}  
\usepackage{setspace}
\usepackage{hyperref, url}
\usepackage[margin=1in]{geometry} %1 inch margins
\usepackage{gensymb}
\usepackage{color}
\usepackage{alltt}
\usepackage{courier}

\usepackage{tikz}
\usetikzlibrary{calc,matrix,trees,shapes,arrows}

%change encoding
%\usepackage[utf8]{inputenc}

%Spacing
\usepackage{setspace}
\onehalfspacing

%code chunk margins
\usepackage{listings}

%macros
\newcommand{\Adv}{{\mathbf{Adv}}}       
\newcommand{\prp}{{\mathrm{prp}}}                  
\newcommand{\calK}{{\cal K}}
\newcommand{\outputs}{{\Rightarrow}}                
\newcommand{\getsr}{{\:\stackrel{{\scriptscriptstyle\hspace{0.2em}\$}}{\leftarrow}\:}}
\newcommand{\andthen}{{\::\;\;}}    %  \: \; for thinspace, medspace, thickspace
\newcommand{\Rand}[1]{{\mathrm{Rand}[{#1}]}}       % A command with one argument
\newcommand{\Perm}[1]{{\mathrm{Perm}[{#1}]}}       
\newcommand{\Randd}[2]{{\mathrm{Rand}[{#1},{#2}]}} % and with two arguments
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\plim}{plim}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcolumntype{Y}{>{\raggedleft\arraybackslash}X}% raggedleft column X

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{CS267 Assignment 2}
\date{\today}
\author{Hussain Al Salem \hspace{5mm} Jason Poulos \hspace{5mm} Yang You \vspace{10mm}}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

In this report, we describe several parallel implementations of a 2D particle simulator and report their performance. The goal is to parallelize code that runs in time $T = O(n)$ on a single processor to run in time $T/p$ when using $p$ processors by taking advantage of shared and distributed memory models. Specifically, we try three implementations: serial code that runs in $O(n)$ time; a MPI distributed memory implementation that runs in $O(n)$ time and $O(n/p$) scaling; and a OpenMP shared memory implementation. For Part 2 of the assignment, we implement in GPU. 

\section{Serial Implementation}

\subsection{Data structures}
Our serial implementation uses a binning structure to execute the interaction step in $O(n)$ time. The 2D grid is divided into small squared bins where each side has the size of cutoff. The size of cutoff was used since it is the maximum distance of interaction. Thus, each particle can only interact with particles in its own square and its 8 neighboring squares on a single iteration. Furthermore, bins that are attached to the edges will have less neighbors and thus they were fixed accordingly. The small bins are stored in a data structure type where each bin has a unique consecutive index. Each bin points to the particles indices contained inside the bin. After each iteration, we empty the bins and update them according to the new particle locations. In the interaction step, each particle only need to check for particles within its bin and neighboring bins. Since the number of particles per square is bound (and equal to 1 on average), the interaction step takes $O(n)$ time.

\subsection{Results}
Figure \ref{fig:serial-on} shows that our serial binning algorithm follows linear scaling ($O(n)$). For comparison, it shows the performance of the naive $O(n^2)$ algorithm on smaller problems.

\begin{figure}
	\centering
 \includegraphics[width=\textwidth]{graphs/serial_binning_vs_naive.png}
  \caption{A plot in log-log scale that shows serial without binning runs on $O(n^2)$ time and serial with binning runs in $O(n)$ time.}
  \label{fig:serial-on}
\end{figure}

\section{OpenMP implementation}

\subsection{Synchronization}
%A description of the synchronization you used in the shared memory implementation.

We implement OpenMP for shared memory parallelization. Synchronization is used after each parallel region in order to prevent race conditions. First, we assign each particle to bins in parallel. Next, we assign each bin to a lock (using \texttt{omp\_lock\_t}). Bin sizes are about equal to the given cutoff size for particle interaction. Thus, the number of bins is generally higher than the number of particles. When we compute forces in each bin in parallel, there will be little contention in most bins because they are empty or relatively sparse. Lastly, we move each particle and clear each bin in parallel for the next timestep. 

\subsection{Results}

Figure \ref{fig:openmp-on} shows considerable speedup of our OpenMP implementation compared to serial binning using 12 threads. We compare based on 12 threads based on the strong scaling measurements in Figure \ref{fig:openmp-strong}. On average, the maximum speed is achieved with 12 threads for number of particles between 500 to 128,000. Four threads is faster below this range, and 24 threads is faster above this range. The simulation time for OpenMP is $O(n^{0.916})$, while the serial is $O(n^{1.256})$. 

\begin{figure}
	\centering
\includegraphics[width=\textwidth]{graphs/serial_binning_vs_openMP_12thr.png}
  \caption{$O(n)$ scaling of the OpenMP implementation with 12 threads compared to the serial implementation.}
  \label{fig:openmp-on}
\end{figure}

\begin{figure}
	\centering
\includegraphics[width=\textwidth]{graphs/openmp_strong_scaling.png}
  \caption{Strong scaling of the OpenMP implementation.}
  \label{fig:openmp-strong}
\end{figure}

\begin{figure}
	\centering
\includegraphics[width=\textwidth]{graphs/weak_scaling.png}
  \caption{Weak scaling of the OpenMP implementation.}
  \label{fig:openmp-weak}
\end{figure}

Figure \ref{fig:openmp-strong} shows the strong scaling of our OpenMP implementation, or how the simulation time varies with the number of threads for a fixed total problem size. OpenMP scales as the number of particles exceeds 32,000. Figure \ref{fig:openmp-weak} shows the weak scaling of OpenMP, or how the computation time varies with the number of threads for a fixed problem size per thread. The slope of the linear scaling at 4,000 particles per thread is 0.088, whereas a slope of 0 would indicate that our code is flawless. There is more communication as the number of threads increases, which explains why the slope is not exactly 0.

\section{MPI Implementation}

\subsection{Communication between nodes}
Let us define $B$ as the number of bins, and $b = \sqrt{B}$. 
We deploy the bins as b $\times$ b distribution, which is just like a grid. Then we divide them by row. Suppose we have p processors, each processor will have $\frac{b^2}{p} = \frac{B}{p}$ bins.
We use the root node (rank is zero) to initiate the particles and then use MPI\_Scatterv to scatter the particles to all the nodes. Concretely, if a certain particle ($pt$) belongs to a bin ($bi$), and $bi$ is owned by a certain node ($nd$), then $pt$ will only be sent to $nd$.
In each step, we need to make the bins in boundary exchange some particles with the bins in neighbor processor's boundary.  We use MPI\_Ibsend to conduct this kind of communication. For more details, please check exchange\_neighbors.
Because we have got the boundary information from the neighboring node, we can conduct force computation on each local node.
Each node will get its own $davg$,  $navg$, $dmin$ variables. Then we will use MPI\_Reduce to get the global variables $rdavg$,  $rnavg$, $rdmin$ by MPI\_SUM, MPI\_SUM, and MPI\_MIN respectively.

\subsection{Results}
The speedup MPI implementation over serial version can be found in Fig. \ref{fig:mpi-speedup}.
The weak scaling results of MPI implementation can be found in Fig. \ref{fig:mpi-weak}.
The strong scaling results of MPI implementation can be found in Fig. \ref{fig:mpi-strong}.
The performance comparison among serial version, OpenMP version, and MPI version can be found in Fig. \ref{fig:serial-openmp-mpi}.
The performance comparison among all the implementations can be found in Fig. \ref{fig:all}.
The experimental setting is the same with the OpenMP version.  On average, the maximum speed is achieved with 24 processes for number of particles between 500 to 128,000. The simulation time for MPI is $O(n^{0.749})$, while OpenMP is $O(n^{0.916})$ and the serial is $O(n^{1.256})$. 

\begin{figure}
  \includegraphics[width=\textwidth]{graphs/MPI_speedup.png}
  %\caption{$O(n)$ scaling of the MPI implementation.}
  \caption{Speedup of MPI implementation.}
  \label{fig:mpi-speedup}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{graphs/MPI_weak_scaling.png}
  \caption{Weak scaling of the MPI implementation.}
  \label{fig:mpi-weak}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{graphs/MPI_strong_scaling.png}
  \caption{Strong scaling of the MPI implementation.}
  \label{fig:mpi-strong}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{graphs/serialbinning_vs_openmp_vs_MPI.png}
  \caption{Serial vs OpenMP vs MPI.}
  \label{fig:serial-openmp-mpi}
\end{figure}

\subsection{Discussion}

From Fig. \ref{fig:serial-openmp-mpi} and Fig. \ref{fig:all} we can observe that MPI achieves better scaling efficiency than OpenMP and GPUs. This means explicit communication behavior does help us to design more scalable software. Note that our implementation is a 1-D partition version, the 2-D partition version may have better scaling efficiency.

The major challenges we faced when writing the MPI code is the communication part, which took us a lot of efforts to manage the sender and receiver in the right way. We use MPI\_Ibsend to conduct a nonblocking buffered sending communication, which is critical for our implementation. In order to design the efficient communication model, we need to re-design some data structures. On the other hand, we use the c++ STL's builtin vector to manage the particle, which is very efficient because we can not predict the number of particles each processor own every step so that we need to dynamically update the array at runtime . Also, debugging MPI code is still non-trivial and tedious. The NERSC tools are useful but most of time we use the printf to get the runtime information.

%\begin{figure}
%  \includegraphics[width=\textwidth]{speedup.png}
%  \caption{Speedup plots that show how closely parallel codes approach the idealized $p$-times speedup.}
%  \label{fig:speedup}
%\end{figure}

%A description of the design choices that you tried and how did they affect the performance.

%A discussion on whether it is possible to do better

%A discussion on using pthreads, OpenMP and MPI.

\section{GPU Implementation} \label{GPU}

In order to reduce simulation time from $O(n^2)$ to $O(n)$, the GPU implementation must support binning. In serial binning, resizable vectors were used to store particles in each bin, but resizing is not supported on the GPU. Therefore, a fixed sized memory allocation was used to store the particles. For this implementation, we chose 32 particles per bin. We also maintained two extra 32-element arrays with the \texttt{bin\_t} structure: \texttt{staying} and \texttt{leaving}. These are used for rebinning when moving particles.

Initially, each particle is assigned to a bin on the driver. Computing the forces occurs in parallel on the GPU, with one thread per bin. Each thread computes the forces between all particles in its bin and each adjacent bin.

The next step is moving particles. This occurs in two steps both of which occur in parallel with one thread per bin. First, each thread moves each particle in its bin by calling \texttt{move\_particle\_gpu}. It then checks with the particle remains in the bin. If it does, it is added to the bin's \texttt{staying} array; otherwise it goes to the \texttt{leaving} array. Second, each thread consolidates the remaining particles in it bin and the outgoing particles destined to the current bin from neighboring bins. This is done by looping over all particles in the \texttt{leaving} arrays.

This method led to adding an extra global synchronization step due to splitting the move operation into two kernels, but it avoids the need for locking since there are no read-write conflicts on the same data. In the first step, each thread only reads and writes its bin, while in the second step, each thread reads neighbors' particle lists but only writes to its bin's \texttt{staying} and \texttt{leaving} arrays, leaving the main particle list uncontended.

Before that, we tried avoiding the need to maintain bins by removing the explicit bins entirely and instead computing the bin of each particle on the fly based on its position. This simplified the move step, since it avoided explicit rebinning. However, it required each thread to examine every particle during the force computation step, which turned out to be orders of magnitude slower than examining only neighboring bins.

\subsection{Results}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphs/gpu_binning_vs_gpu_serial.png}
  \caption{Log-log scale plot that shows GPU with binning versus GPU with binning performance.}
  \label{fig:gpu-compare}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphs/serial_openmp_gpu.png}
  \caption{Log-log scale plot that shows Serial with binning, GPU with binning and OpenMP performance.}
  \label{fig:gpu-all}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphs/all.png}
  \caption{Comparison among all the implementations (Serial vs OpenMP vs MPI vs GPU).}
  \label{fig:all}
\end{figure}

We benchmarked the the GPU Naive Code and our GPU binning implementation on the Stampede GPU architecture. Figure~\ref{fig:gpu-compare} shows that our implementation is $O(n^{0.8673})$ fast compared to the naive GPU implementation which is $O(n^{1.3691})$ fast. Figure~~\ref{fig:gpu-all} shows that the GPU code has slightly better performance than the OpenMP. The GPU code is $O(n^{0.8673})$ fast while the OpenMP code is $O(n^{0.9161})$ fast. 

\subsection{Discussion}

Though CUDA allows writing GPU code in a subset of C, its libraries provide much less functionality than STL. In particular, there is no resizable, concurrent vector data structure, which would have made the particle simulation code much simpler to write, though slower than the lock-free approach we ultimately settled on.

Additionally, the GPU architecture is well-suited for high-intensity applications, but binning substantially increased the memory bandwidth needs of our particle simulator, lowering its intensity and reducing the speedup due to the GPU.

We therefore conclude that, for applications like particle simulation, the performance gains of utilizing the GPU are not worth the substantially increased programming effort compared to OpenMP, which required only a few lines of modifications to the serial code.


%Appendix
%\pagebreak
%\begin{appendices}
%
%\end{appendices}

\end{document}
